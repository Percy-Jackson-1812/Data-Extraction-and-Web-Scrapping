{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3d3048f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\requests\\__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (5.1.0)/charset_normalizer (2.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "# Import all the required libraries.\n",
    "\n",
    "import csv\n",
    "import bs4\n",
    "import nltk\n",
    "import string\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f6a146d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Input.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# input.xlsx is converted to input.csv and uploaded to dataframe.\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mInput.xlsx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m df\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_base.py:457\u001b[0m, in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, decimal, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[0;32m    456\u001b[0m     should_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 457\u001b[0m     io \u001b[38;5;241m=\u001b[39m \u001b[43mExcelFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine \u001b[38;5;241m!=\u001b[39m io\u001b[38;5;241m.\u001b[39mengine:\n\u001b[0;32m    459\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    460\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    461\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    462\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_base.py:1376\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[1;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[0;32m   1374\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxls\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1375\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1376\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[43minspect_excel_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1377\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[0;32m   1378\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1379\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1380\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1381\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExcel file format cannot be determined, you must specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1382\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man engine manually.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_base.py:1250\u001b[0m, in \u001b[0;36minspect_excel_format\u001b[1;34m(content_or_path, storage_options)\u001b[0m\n\u001b[0;32m   1247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content_or_path, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m   1248\u001b[0m     content_or_path \u001b[38;5;241m=\u001b[39m BytesIO(content_or_path)\n\u001b[1;32m-> 1250\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m   1252\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[0;32m   1253\u001b[0m     stream \u001b[38;5;241m=\u001b[39m handle\u001b[38;5;241m.\u001b[39mhandle\n\u001b[0;32m   1254\u001b[0m     stream\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py:798\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    789\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    790\u001b[0m             handle,\n\u001b[0;32m    791\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    794\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    795\u001b[0m         )\n\u001b[0;32m    796\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    797\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 798\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    799\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    801\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Input.xlsx'"
     ]
    }
   ],
   "source": [
    "# input.xlsx is converted to input.csv and uploaded to dataframe.\n",
    "\n",
    "df = pd.read_excel('Input.xlsx')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80655204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nan is dropped from dataframe\n",
    "\n",
    "df = df.dropna()\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1332de19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL present in the dataframe is copied to url_list in form of a list.\n",
    "\n",
    "url_list = [url for url in df['URL']]\n",
    "\n",
    "# The URL present in url_list are accessed by chrome and complete HTML code for said page is stored in text form and in a list \n",
    "# called text and according to the serial in which url was saved in url_list.\n",
    "\n",
    "text = []\n",
    "for url in url_list:\n",
    "    text.append(requests.get(url,headers={\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7a414e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The unrequired part of HTML code present in text is removed using .content,'html.parser'.\n",
    "\n",
    "for i in range(len(text)):\n",
    "    text[i] = bs4.BeautifulSoup(text[i].content,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56413e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The required paragraph is copied to list called articles by finding and specifying the class in HTML under which \n",
    "# the paragraph is present.\n",
    "\n",
    "articles = []\n",
    "for text in text:\n",
    "    articles.append(text.find('div', attrs = {'class':'td-post-content'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e356ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# articles list is checked for Null value and their position.\n",
    "\n",
    "for i in range(len(articles)):\n",
    "    if(articles[i] == None):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f491de60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 3 indexes at which null value were found are dropped from database to prevent error in further code.\n",
    "\n",
    "df = df.drop([df.index[7], df.index[20],df.index[107]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967e5083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The null values are dropped from articles list and copied to a new list called new_article.\n",
    "\n",
    "new_article = []\n",
    "for val in articles:\n",
    "    if val != None :\n",
    "        new_article.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bad9423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML code is completely removed by using \".text\" and \\n present after removing the code is replaced by space(' '). \n",
    "# The remaining data in new_article is the data required for sentiment analysis.\n",
    "\n",
    "for i in range(len(new_article)):\n",
    "    new_article[i]= new_article[i].text.replace('\\n',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e4aa44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A new list called stop_words is formed and the stop words provided in form of multiple text files are added to stop_words \n",
    "# list.\n",
    "\n",
    "stop_words = []\n",
    "\n",
    "stopWordsFile1 = 'StopWords_Auditor.txt'\n",
    "for stop_word in open(stopWordsFile1, 'r').readlines():\n",
    "    stop_words.append(stop_word.rstrip())\n",
    "\n",
    "stopWordsFile2 = 'StopWords_Currencies.txt'\n",
    "for stop_word in open(stopWordsFile2, 'r').readlines():\n",
    "    stop_words.append(stop_word.rstrip())\n",
    "\n",
    "stopWordsFile3 = 'StopWords_Generic.txt'\n",
    "for stop_word in open(stopWordsFile3, 'r').readlines():\n",
    "    stop_words.append(stop_word.rstrip())\n",
    "\n",
    "stopWordsFile4 = 'StopWords_GenericLong.txt'\n",
    "for stop_word in open(stopWordsFile4, 'r').readlines():\n",
    "    stop_words.append(stop_word.rstrip())\n",
    "\n",
    "stopWordsFile5= 'StopWords_DatesandNumbers.txt'\n",
    "for stop_word in open(stopWordsFile5, 'r').readlines():\n",
    "    stop_words.append(stop_word.rstrip())\n",
    "\n",
    "stopWordsFile6= 'StopWords_Geographic.txt'\n",
    "for stop_word in open(stopWordsFile6, 'r').readlines():\n",
    "    stop_words.append(stop_word.rstrip())\n",
    "    \n",
    "stopWordsFile7= 'StopWords_Names.txt'\n",
    "for stop_word in open(stopWordsFile7, 'r').readlines():\n",
    "    stop_words.append(stop_word.rstrip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f358f49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It saves the different number of sentences present in differet paragraphs present in new_article list. \n",
    "# The number of sentences is further saved in a new list called sentences.  \n",
    "\n",
    "sentences = []\n",
    "for article in new_article:\n",
    "    sentences.append(len(sent_tokenize(article)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27e2118",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_articles = [' ']*len(new_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b55dffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Markings such as '?', '.',',' and '!' are replaced with space(' '). \n",
    "\n",
    "for i in range(len(new_article)):\n",
    "    for w in stop_words:\n",
    "        cleaned_articles[i]= new_article[i].replace('?',' ').replace('.',' ').replace(',',' ').replace('!',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b521ff02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A new list called words is created and number of words present in the differet paragraphs is present in new_article list\n",
    "# is saved into the words list\n",
    "\n",
    "words = []\n",
    "for article in new_article:\n",
    "    words.append(len(word_tokenize(article)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e2ce36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A new list called words_cleaned is used to store number of words from cleaned_articles.\n",
    "\n",
    "words_cleaned = []\n",
    "for article in cleaned_articles:\n",
    "    words_cleaned.append(len(word_tokenize(article)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f14252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positive words and negative words are copied from positive-words.txt and negative-words.txt respectively and \n",
    "# stored in new lists called positive_words and negative_words respectively. Further positive score and negative\n",
    "# score are calculated by finding postive words and negative words in respectively in the cleaned_article list.\n",
    "\n",
    "positive_words = []\n",
    "negative_words = []\n",
    "\n",
    "positiveWordsFile = 'positive-words.txt'\n",
    "for positive_word in open(positiveWordsFile, 'r').readlines():\n",
    "    positive_words.append(positive_word.rstrip())\n",
    "    \n",
    "positive_score = [0]*len(new_article)\n",
    "for i in range(len(new_article)):\n",
    "    for word in positive_words:\n",
    "        for letter in cleaned_articles[i].lower().split(' '):\n",
    "            if letter==word:\n",
    "                positive_score[i]+=1\n",
    "\n",
    "negativeWordsFile = 'negative-words.txt'\n",
    "for negative_word in open(negativeWordsFile, 'r').readlines():\n",
    "    negative_words.append(negative_word.rstrip())\n",
    "    \n",
    "negative_score = [0]*len(new_article)\n",
    "for i in range(len(new_article)):\n",
    "    for word in negative_words:\n",
    "        for letter in cleaned_articles[i].lower().split(' '):\n",
    "            if letter==word:\n",
    "                negative_score[i]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6589498a",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_cleaned = np.array(words_cleaned)\n",
    "sentences = np.array(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98ea8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['POSITIVE SCORE'] = positive_score\n",
    "df['NEGATIVE SCORE'] = negative_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25832325",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['POLARITY SCORE'] = (df['POSITIVE SCORE']-df['NEGATIVE SCORE'])/ ((df['POSITIVE SCORE'] +df['NEGATIVE SCORE']) + 0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35014c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SUBJECTIVITY SCORE'] = (df['POSITIVE SCORE'] + df['NEGATIVE SCORE'])/( (words_cleaned) + 0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73ee481",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['AVG SENTENCE LENGTH'] = np.array(words)/np.array(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a12c9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex words and syllables are counted here for calculating percentage of complex words, fog index, complex word count \n",
    "# and syllables per word.\n",
    "\n",
    "complex_words = []\n",
    "syllables_counts = []\n",
    "for article in new_article:\n",
    "    syllables_count = 0\n",
    "    d=article.split()\n",
    "    ans=0\n",
    "    for word in d:\n",
    "        count=0\n",
    "        for i in range(len(word)):\n",
    "            if(word[i]=='a' or word[i]=='e' or word[i] =='i' or word[i] == 'o' or word[i] == 'u'):\n",
    "                count+=1\n",
    "            if(i==len(word)-2 and (word[i]=='e' and word[i+1]=='d')):\n",
    "                count-=1\n",
    "            if(i==len(word)-2 and (word[i]=='e' and word[i]=='s')):\n",
    "                count-=1\n",
    "        syllables_count+=count   \n",
    "        if(count>2):\n",
    "            ans+=1\n",
    "    syllables_counts.append(syllables_count)\n",
    "    complex_words.append(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb8c83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['PERCENTAGE OF COMPLEX WORDS'] = np.array(complex_words)/np.array(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1efcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['FOG INDEX'] = 0.4 * (df['AVG SENTENCE LENGTH'] + df['PERCENTAGE OF COMPLEX WORDS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7023e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['AVG NUMBER OF WORDS PER SENTENCES'] = df['AVG SENTENCE LENGTH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c87b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['COMPLEX WORD COUNT'] = complex_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a079164",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['WORD COUNT'] = words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6786d167",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['WORD COUNT'] = words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37487de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of characters are calculated for every paragraph and stored in list called total_characters.\n",
    "\n",
    "total_characters = []\n",
    "for article in new_article:\n",
    "    characters = 0\n",
    "    for word in article.split():\n",
    "        characters+=len(word)\n",
    "    total_characters.append(characters) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8061db61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of personal noun are counted and stored in a list called personal_nouns.\n",
    "\n",
    "personal_nouns = []\n",
    "personal_noun =['I', 'we','my', 'ours','and' 'us', 'We','My', 'Ours','And' 'Us'] \n",
    "for article in new_article:\n",
    "    ans=0\n",
    "    for word in article:\n",
    "        if word in personal_noun:\n",
    "            ans+=1\n",
    "    personal_nouns.append(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd117f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['PERSONAL PRONOUN'] = personal_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e76c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['AVG WORD LENGTH'] = np.array(total_characters)/np.array(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da98cf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final dataframe is printed.\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead0bee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataframe prepared is stored in a CSV file called ASSIGNMENT SOLUTION.csv. \n",
    "\n",
    "df.to_csv('ASSIGNMENT SOLUTION.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cf7d01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
